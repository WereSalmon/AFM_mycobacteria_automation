{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06f02f4",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad7817",
   "metadata": {},
   "source": [
    "Long term time lapse atomic force microscopy (LTTL-AFM) is a groundbreaking new technique for imaging bacteria in high resolution, developped by Dr Alexander Eskandarian. It allows us to observe how physical properties of the bacterial cell surface, detected by high resolution AFM, change over time. We can then examine how those properties change around important events such as cell division. Eskandarian et al (2017), Hannebelle et al (2020), and Odermatt et al (2020) have all identified such properties by examining this data manually including: wave troughs which division events localize to, furrows which occur several minutes before the development of the FTSZ ring, changes in stiffness on the cell surface, and nano-vesicles appearing on the cell surface. This technique has also made it easy to observe the growth of bacterial cells much more accurately than in previous work (Hannebelle et al, 2020), and we may even relate this growth to the previous physical features  mentioned. \n",
    "\n",
    "This project aims to automate the process of finding and detecting biophysical features in images of several cells, and thus produce a large and accurate dataset of them for further study. We would also like to automate the measurement of cell growth and divsion events, both to study them and relate them to relevant biophysical features. This requires images to be processed, aligned, and segmented into masks which must have false masks filtered out, be tracked over time and have their biophysical features identified. Building on a project by Keegan Flannagan and Hasti Delfi several different programs are used to porcess and segment the images, and skeletonize and analyze the masks of cells in them.\n",
    "\n",
    "Gwyddion (an external desktop program), Cellpose (a neurel-net based python package which segments and creates masks of cells), PyTracker (a program which implements single particle tracking to track cells over time), SciKit Image (a python image processing package which produces skeletons of masks), and a pole-based method for turning skeletons into centerlines of the cell. Then division events can be identified and biophysical features of the cell can be tracked along the centerline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc1ea3",
   "metadata": {},
   "source": [
    "## 2. Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef00160",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73520352",
   "metadata": {},
   "source": [
    "## 3. Analysis\n",
    "\n",
    "### Setup\n",
    "\n",
    "by Keegan Flannagan and Hasti Delfi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf38df",
   "metadata": {},
   "source": [
    "The easiest way to ensure that all the necessary packages and required dependencies for this analysis are installed is to run this analysis within a conda environment. If you are unfamiliar with miniconda, you can learn more about it and download it from the following link https://docs.conda.io/en/latest/miniconda.html.\n",
    "\n",
    "A conda environment with the complete version of Cellpose pre-installed can be created and activated by entering the following commands into the terminal:\n",
    "```\n",
    "conda create --name cellpose python=3.8\n",
    "conda activate cellpose\n",
    "```\n",
    "Once the Cellpose conda environment has been activated, additional required packages can be installed via pip using the following commands:\n",
    "\n",
    "```\n",
    "pip install jupyterlab\n",
    "pip install matplotlib\n",
    "pip install pandas\n",
    "pip install imutils\n",
    "pip install imageio\n",
    "pip install fil_finder\n",
    "pip install radfil\n",
    "pip install skimage\n",
    "pip install networkx\n",
    "pip install sklearn\n",
    "pip install astropy\n",
    "pip install sknw\n",
    "```\n",
    "After all of these installations are complete and this notebook is opened within the Cellpose conda environment, the example analysis is ready to be run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec75dd8",
   "metadata": {},
   "source": [
    "### Loading in necessary packages and defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from cellpose import utils, io, models, plot\n",
    "from scipy.spatial import distance as dist\n",
    "import re\n",
    "import tools\n",
    "from radfil import radfil_class, styles\n",
    "from astropy import units as u\n",
    "import imageio.v2 as imageio\n",
    "import copy\n",
    "from PIL import Image\n",
    "from fil_finder import FilFinder2D\n",
    "from shapely.geometry import Polygon\n",
    "from skimage.util import invert\n",
    "from skimage.morphology import skeletonize\n",
    "from cv2 import imwrite\n",
    "from tools import prune2\n",
    "from scipy.interpolate import splprep\n",
    "from scipy.interpolate import splev\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "date = \"08-10-2015\"\n",
    "my_data = \"../data/\" + date + \"/Height/\"\n",
    "my_pfe = \"../data/\" + date + \"/Peak_Force_Error/\"\n",
    "my_apm = \"../data/\" + date + \"/Amplitude/\"\n",
    "my_sti = \"../data/\" + date + \"/Stiffness/\"\n",
    "segments_path = \"../cellpose_outputs/\" + date + \"/Height/\"\n",
    "cell_path = \"Individual_cells/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c8586",
   "metadata": {},
   "source": [
    "## 3.1 Segmentation\n",
    "\n",
    "by Keegan Flannagan and Hasti Delfi\n",
    "\n",
    "### 3.1.1 Load and sort the initial image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up a list of input files from our example data.\n",
    "files = os.listdir(my_data)\n",
    "for fichier in files[:]:\n",
    "    if not(fichier.endswith(\".png\")):\n",
    "        files.remove(fichier)\n",
    "        \n",
    "# Sort files by timepoint.\n",
    "files.sort(key = tools.natural_keys)      \n",
    "\n",
    "# Create a list that contains the full path to each of our image files. \n",
    "save_names = [segments_path + file for file in files]\n",
    "file_names = [my_data + file for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b064710",
   "metadata": {},
   "source": [
    "Each image in our example dataset consists of a grayscaled height image wih a scale bar at the side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96046075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an example image.\n",
    "img = io.imread(file_names[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811095df",
   "metadata": {},
   "source": [
    "### 3.1.2 Run Cellpose\n",
    "\n",
    "Cellpose is a program which uses a generalist model to perform cell or nucleus segmentation. Cellpose utilizes a deep neural network which was trained on a diverse set of images which comprised over 70,000 individual segmented  objects. This diverse training set allows Cellpose to perform segmentation on a wide range of image types without previous training or parameter adjustment. We are going to use this pre-trained cytoplasm model for our segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that the cytoplasm Cellpose model for the segmentation. \n",
    "model = models.Cellpose(gpu=False, model_type='cyto')\n",
    "\n",
    "# define CHANNELS to run segementation on\n",
    "# grayscale=0, R=1, G=2, B=3\n",
    "# channels = [cytoplasm, nucleus]\n",
    "channels = [0,0]\n",
    "\n",
    "# Loop over all of our image files and run Cellpose on each of them. \n",
    "for filename, savename in zip(file_names, save_names):\n",
    "    img = io.imread(filename)\n",
    "    masks, flows, styles, diams = model.eval(img, diameter = 80, channels=channels, flow_threshold = 0.8)\n",
    "\n",
    "    # save results so you can load in gui\n",
    "    io.masks_flows_to_seg(img, masks, flows, diams, savename, channels)\n",
    "\n",
    "    #save results as png\n",
    "    io.save_to_png(img, masks, flows, savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ea288",
   "metadata": {},
   "source": [
    "Below we can see an illustrative example of Cellpose's outputs. Each cell in the original image has an associated outline and mask which cover the perimeter and area of the cell respectively. The image to the far right shows the predicted vector fields for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of Cellposes output. \n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plot.show_segmentation(fig, img, masks, flows[0], channels=channels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0676f",
   "metadata": {},
   "source": [
    "The following plot shows the vector flow fields generated by Cellpose. These flow fields are what Cellpose uses to deliminate cells from each other. One can observe that all of the pixels within the individual cells follow a gradient to a single cell centerpoint which is visible as a black dot. This allows Cellpose to map each individual pixel to the center of the cells which is how Cellpose decides if a pixel is inside of a cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d197255",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = imageio.imread(\"flow_legend.jpg\")\n",
    "fig, ax = plt.subplots(1,2, gridspec_kw={'width_ratios': [3, 1]})\n",
    "ax[1].axis(\"off\")\n",
    "ax[0].imshow(flows[0])\n",
    "ax[1].imshow(legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a628d4f",
   "metadata": {},
   "source": [
    "## 3.2 Collect metadata\n",
    "\n",
    "After the cells in our images have been properly segmented, we will want to analyze the masks and outlines outputted by Cellpose in order to collect data on several cell parameters over time. The parameters we are interested in includes the perimeter of the cell, the area of the cell, the degree to which a cell touches another cell, the location of the cell, the hight along the cells medial axis or skeleton, and the radial profile. We would also like to analyze how these parameters change over time for each cell, so a simple cell tracking algorithm has been implemented in order to keep track of each cell over time. \n",
    "\n",
    "### 3.2.1 Load up segmentation files\n",
    "\n",
    "by Keegan Flannagan and Hasti Delfi\n",
    "\n",
    "The segmentation files generated in the last step can now be loaded back into Jupyter for analysis. Each of the images and associated segmentation files from the example dataset contain a timepoint within their file names. This makes it easy to sort the data from earliest to latest which will be important for cell tracking and for the organization of the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames for our segmentation datasets. \n",
    "segmented = os.listdir(segments_path)\n",
    "for fichier in segmented[:]: \n",
    "    if not(fichier.endswith(\".npy\")):\n",
    "        segmented.remove(fichier)\n",
    "\n",
    "# Sort the filenames by timepoint. \n",
    "segmented.sort(key = tools.natural_keys)\n",
    "\n",
    "# Create a list that records the timepoints for every image. \n",
    "time_list = []\n",
    "for segment in segmented:\n",
    "    time_list.append(int(re.findall(r'\\d+', segment)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5999",
   "metadata": {},
   "source": [
    "Before the desired cell parameters are estimated, small cell masks/outlines are removed from the segmentation datasets. This is done because small masks are often the result of Cellpose picking up on small visual artifacts present in the AFM data. This simple size filter removes the vast majority of these artifacts. The original image and the cell outlines are then extracted from each of the original segmentation datasets. These outputs will be sufficient to calculate all of the additional parameters needed for the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists.\n",
    "outl_temp = []\n",
    "height_img_list = []\n",
    "\n",
    "# Fill lists with img and outline data from each segment\n",
    "for segment, time in zip(segmented, time_list):\n",
    "    # Load up the segmentation data. \n",
    "    dat = np.load(segments_path + segment, allow_pickle=True).item()\n",
    "    \n",
    "    # Remove small masks from the data\n",
    "    dat[\"masks\"] = utils.fill_holes_and_remove_small_masks(dat[\"masks\"], min_size = 800)\n",
    "    \n",
    "    # Populate list with each cell outline.\n",
    "    outl = utils.outlines_list(dat['masks'])\n",
    "    outl_temp.append(outl)\n",
    "    \n",
    "    # Populate list with each original image.\n",
    "    image = dat[\"img\"]\n",
    "    height_img_list.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8089e73",
   "metadata": {},
   "source": [
    "The code below shows an illustrative example of a cell with its associated outline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dfcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the original image\n",
    "plt.imshow(height_img_list[0])\n",
    "# plotting the outlines. \n",
    "for o in outl_temp[0]:\n",
    "    plt.plot(o[:,0], o[:,1], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image dictionary contains the complete images for all of the image types. \n",
    "img_dict = {\n",
    "    \"Height\": height_img_list,\n",
    "    }\n",
    "\n",
    "# The metadata dictionary will hold all of the data we want to save into our final table.\n",
    "metadata_dict = {}\n",
    "\n",
    "# The individual cell dictionary will save information for each individual cell such as the outline, skeleton, cropped image, etc. \n",
    "ind_cell_dict = {}\n",
    "\n",
    "# The structural dictionary will hold information on the timepoint of each image, the cell IDs for each cell in each image,\n",
    "# and the outlines and bounding boxes for each cell in each image.\n",
    "structural_dict = {\n",
    "    \"Time\": time_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cc9cd",
   "metadata": {},
   "source": [
    "### 3.2.2 Create bounding boxes for each cell outline\n",
    "\n",
    "by Keegan Flannagan and Hasti Delfi\n",
    "\n",
    "A bounding box is the smallest possible rectangle that can contain all of the points within a shape. Creating bounding boxes for each of our cell outlines will allow for the quick computation of the center points (centroids) of each cell which is necessary for cell tracking. Furthermore, detecting bounding box intersections can be used as a quick way to determine if two cells might be touching each other which allows for an optimization of our cell overlap measuring algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list of boxes\n",
    "boxes_temp = []\n",
    "box_img = copy.deepcopy(height_img_list)\n",
    "\n",
    "# Fill lists with the coordinates of bounding boxes for each cell outline.\n",
    "for outl, img in zip(outl_temp, box_img):\n",
    "    boxes = tools.get_boxes(outl)\n",
    "    boxes_temp.append(boxes)\n",
    "    for box in boxes:\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        cv2.drawContours(img, [box.astype(\"int\")], -1, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fe679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example to illustrate the bounding boxes. \n",
    "plt.imshow(box_img[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945be9d",
   "metadata": {},
   "source": [
    "### 3.2.3 Cell Tracking\n",
    "\n",
    "by Samuel Munn\n",
    "\n",
    "We execute single particle tracking algorithms on the centroids of masks to find the trajectories of cells in images. These methods measure the probability of a particle in an initial image moving to the position of a particle in a second image via brownian motion. It then it finds the most probable path over a short series of images, accounting for the fact that particles may dissappear in some frames. We use the method described in () as implemented in the Trackpy package by ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.DataFrame(columns=['frame','y','x'])\n",
    "for i in range(0,len(centerl_aligned[:7])):\n",
    "    for center in centerl_aligned[i]:\n",
    "        df2=pd.DataFrame({'frame':[i],'y':[center[0]],'x':[center[1]]})\n",
    "        C = pd.concat([C,df2],ignore_index=True, axis=0)\n",
    "P = trackpy.link(C,40,memory=3)\n",
    "pf = trackpy.filter_stubs(P,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74de7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our new lists to the proper dictionaries. \n",
    "structural_dict[\"IDs\"] = IDs_list\n",
    "structural_dict[\"outlines\"] = outl_list\n",
    "structural_dict[\"bounding_boxes\"] = boxes_list\n",
    "\n",
    "metadata_dict[\"centroids\"] = centers_list\n",
    "\n",
    "# Destroy the temporary lists to get some more space. \n",
    "boxes_temp.clear()\n",
    "outl_temp.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd835d5",
   "metadata": {},
   "source": [
    "The below code outputs a series of images which illustrate the cell tracking process. One can observe new cell IDs being assigned as cell division events occur. These new IDs are then subsequently tracked over multiple images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outputs to see how the cells are tracked over time.\n",
    "fig, ax = plt.subplots(2,2, figsize = (15,8))\n",
    "ax[0][0].imshow(box_img[2])\n",
    "ax[0][1].imshow(box_img[14])\n",
    "ax[1][0].imshow(box_img[20])\n",
    "ax[1][1].imshow(box_img[26])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700931e",
   "metadata": {},
   "source": [
    "### 3.2.4 Determining the perimeter, area, and overlap of cells\n",
    "\n",
    "by Keegan Flannagan\n",
    "\n",
    "The first piece of metadata that will be collected for each cell within each image is the perimeter size, area, and degree of cell overlap. Perimeter and area can be quickly and easily calculated using the outlines outputted by Cellpose. The degree to which each cell overlaps with other cells can be calculated using the following algorithm:\n",
    "1. Take the bounding box for one of the cells in the image. \n",
    "2. Detect if the bounding box from that cell intersects with any of the bounding boxes from the other cell in the image. \n",
    "3. If the bounding boxes do intercept, compare the distance between the pixels from the outlines. If the distance between the pixels is one, then add 1 pixel to the amount of overlap for that cell. \n",
    "4. Repeat steps 1-3 for every cell in every image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict[\"Perimeter\"], metadata_dict[\"Area\"] = tools.peri_area(outl_list)\n",
    "\n",
    "metadata_dict[\"Overlap\"] = tools.get_overlap(outl_list, boxes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eda779",
   "metadata": {},
   "source": [
    "### 3.2.5 Save individual cells and their outlines\n",
    "\n",
    "by Keegan Flannagan\n",
    "\n",
    "In this protocol, skeletonization is completed using the MATLAB function bwmorph. As such, it is necessary to save each individual cell from each image as its own isolated image and mask so that these masks can be used as inputs for the MATLAB code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_cell_dict[\"Height\"], ind_cell_dict[\"Mask\"] = tools.extract_ind_cells(IDs_list, outl_list, img_dict[\"Height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3362b",
   "metadata": {},
   "source": [
    "### 3.2.6 Skeletonization of individual cells\n",
    "\n",
    "by Samuel Munn\n",
    "\n",
    "The algorithm takes the individual cell masks (Figure) as inputs. It then creates the topological skeleton of the mask's shape using the skeletonize function in skimage.morphology, Python's Scikit Image package. skeletonize uses Zhang's algorithm to create the skeleton from a binary image. (Zhang, 1984) This creates the initial skeleton, which often splits into two or more branches at either end of the cell (Figure). This workflow solves this problem by finding the poles of the cell and then extending the skeleton to those poles. This can be completed using the function \"prune2\" in tools.py.\n",
    "\n",
    "To find the poles of the cell first the \"explore_poles\" function in tools.py finds the set $S$ of extreme points in the mask as follows: starting with a smoothed curve (a periodic 3-spline) fitted to the points in the outline we find the radius at any given point on the outline as the distance from the outline to it's centroid. We then find the set of extreme points to be all local maxima of this radial distance function. We then divide $S$ into two subsets based on which side of the short axis of bounding box the extreme points fall on. We will call these subsets $S_1$ and $S_2$. The poles $P_1$ and $P_2$ are then defined as the weighted averages of their respective subsets, weighted by the radii of the extreme points.\n",
    "\n",
    "prune2 then executes an algorithm  then crop all skeletons which do not intersect the outline near the poles, and choose the longest path through the skeleton to be the basis for the centerline. We then fit a smooth curve to this path (a 3-spline), ending at the poles nearest to it's ends. If the curvature does not exceed a certain threshold (standard threshold is 0.2) then it is converted into a binary image and returned as the centerline. If the centerline's cuvature does exceed this threshold the ends of the path through the skeleton are cropped, and the process of fitting a spline repeated. This new spline is then converted to a binary image and returned as the centerline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of skeletons (before and after branch pruning) with associated timepoints\n",
    "unpruned_skel_list=[]\n",
    "skel_list = []\n",
    "#list of masks with skeletons overlayed in black\n",
    "over_list=[]\n",
    "#lists of poles, lengths, and centroids\n",
    "pole_list = []\n",
    "length_list = []\n",
    "centroid_list = []\n",
    "for time, mask_set in zip(structural_dict[\"Time\"], ind_cell_dict[\"Mask\"]):\n",
    "    unpruned_skel_set = []\n",
    "    skel_set = []\n",
    "    over_set = []\n",
    "    length_set = []\n",
    "    pole_set = []\n",
    "    centroid_set = []\n",
    "    #create skeletons; save masks, skeletons, and masks with skeletons as lists, save skeletons and skeletons with masks as images\n",
    "    for mask in mask_set:\n",
    "        new_mask=mask[:,:,0]>0\n",
    "        unpruned_skel = tools.padskel(mask)\n",
    "        outline = utils.masks_to_outlines(new_mask)\n",
    "        long = np.shape(unpruned_skel)[0]<np.shape(unpruned_skel)[1]\n",
    "        poles,centroid = tools.explore_poles(outline,long)\n",
    "        skel,length,pts,s = tools.prune2(unpruned_skel,outline,new_mask,poles,sensitivity=5)\n",
    "        over = invert(skel+invert(new_mask)) #the mask overlayed on the skeleton (boolean array)\n",
    "        pole_set.append(poles)\n",
    "        unpruned_skel_set.append(unpruned_skel)\n",
    "        skel_set.append(skel)\n",
    "        over_set.append(over)\n",
    "        length_set.append(length)\n",
    "    pole_list.append(pole_set)\n",
    "    unpruned_skel_list.append(unpruned_skel_set)\n",
    "    skel_list.append(skel_set)\n",
    "    over_list.append(over_set)\n",
    "    length_list.append(length_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to dictionaries. \n",
    "ind_cell_dict[\"Skeleton\"], ind_cell_dict[\"Skel + Mask\"], ind_cell_dict[\"Unpruned\"] = skel_list, over_list, unpruned_skel_list\n",
    "metadata_dict[\"Length\"] = length_list\n",
    "metadata_dict[\"Poles\"] = pole_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "[start_pole,end_pole]=metadata_dict[\"Poles\"][12][1]\n",
    "skel = ind_cell_dict['Skel + Mask'][12][1]\n",
    "plt.imshow(skel,cmap=plt.cm.gray)\n",
    "plt.plot(start_pole[0],start_pole[1],'r.')\n",
    "plt.plot(end_pole[0],end_pole[1],'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e1c12",
   "metadata": {},
   "source": [
    "### 3.2.7 Using RadFil to build and analyze the radial profile\n",
    "\n",
    "by Keegan Flannagan\n",
    "\n",
    "RadFil is a python package that was originally designed to measure the radial profiles of interstellar filaments, but can be readily applied to other structures such as cells (Zucker et al., 2018). Radfil was chosen for this analyses because it grants the user full control over the parameters used to build and fit the radial profiles and because of the convenient way it packages the output data.\n",
    "\n",
    "RadFil takes the cell image, mask, and skeleton from each isolated cell as input. It then performs spline interpolation by fitting a b-spline to the original skeleton which creates a smoothed, continuous version of the original skeleton. RadFil then creates a series of cuts at even points along the skeleton which are perpendicular to the tangent of the skeleton at each cut point. These perpendicular cuts extend to the edge of the cell mask, and can therefore be used to compute the radius or diameter of the cell at every point along the skeleton. RadFil also saves the pixel intensities along the smoothed skeleton, which can be used to calculate the height along the skeleton which is also known as the ridgeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22817766",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv = 3.9215686274509802\n",
    "s_conv = 392.15686274509802\n",
    "p_conv = 1\n",
    "# There is currently insufficient information supplied with the image files to convert the PFE pixel intensities to useful units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# For each image file we are analyzing, we are going to use RadFil to create a RadFil object and then extract the distance\n",
    "# And pixel intensity profiles from each cell. \n",
    "metadata_dict[\"Intensity Profile\"], metadata_dict[\"Ridgeline\"], metadata_dict[\"Distance Profile\"], metadata_dict[\"Width\"] = tools.apply_radfil(ind_cell_dict[\"Height\"],\n",
    "                                                                                                                                         ind_cell_dict[\"Mask\"], \n",
    "                                                                                                                                         ind_cell_dict[\"Skeleton\"],\n",
    "                                                                                                                                        conv = 3.9215686274509802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cell image\n",
    "fil_image = ind_cell_dict[\"Height\"][3][0][:,:,0]\n",
    "            \n",
    "# Load the cell mask and convert to a boolean array. \n",
    "fil_mask = ind_cell_dict[\"Mask\"][3][0][:,:,0]>0\n",
    "\n",
    "# Load the cell skeleton. \n",
    "fil_skeleton = ind_cell_dict[\"Skeleton\"][3][0]>0\n",
    "fil_skeleton[0,:] = False\n",
    "            \n",
    "# Use RadFil to create a radial profil object and then append this object to a list.\n",
    "# We will use the data stored in this object to calculate height and radial profile along the\n",
    "# medial axis. \n",
    "radobj=radfil_class.radfil(fil_image, mask=fil_mask, filspine=fil_skeleton, distance=200)\n",
    "radobj.build_profile(samp_int=1, shift = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d98106",
   "metadata": {},
   "source": [
    "### 3.2.8 Organize everything into metadata tables\n",
    "\n",
    "by Keegan Flannagan\n",
    "\n",
    "Once all of the data has been collected for each individual image and cell, the data needs to be organized into a useful format. The following code loops through every cell ID that was assigned during cell tracking. For each Cell ID, it then loops through all of the data lists we have created and places all of the data for that image/timepoint into a row. If there was no data for a certain timepoint, the row is left empty. The result of this is a table for each cell that shows when that cell appeared/dissapeared along with its perimeter size, area, degree of overlap, center position, radial profile, ridgeline, and skeleton length at each timepoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the last ID which was assigned during the cell tracking.\n",
    "max_ID = tools.get_max_ID(structural_dict[\"IDs\"])+1\n",
    "\n",
    "# For each cell that was given an ID, Create a table of metadata. \n",
    "for i in range(max_ID):\n",
    "    df = tools.get_metadata(metadata_dict, structural_dict, i)\n",
    "    df.to_csv(\"metadata/05-10-2015/cell_id_\" + str(i) + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83cbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = tools.get_metadata(metadata_dict, structural_dict, 0)\n",
    "ex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1de946",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = metadata_dict['Ridgeline'][12][1]\n",
    "s = np.linspace(0,1,len(profile))\n",
    "plt.plot(s,profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bc567",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "by Keegan Flannagan and Samuel Munn\n",
    "\n",
    "## 4.1 Height along the centerline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7836019",
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to demonstrate the effectiveness of this automation method, we replicated figure 1a) from Eskandarian et al. 2017 using the data collected by our protocol.\n",
    "\n",
    "The first step in generating these plot is to use our get_metadata function to get the data from cell zero and cell 2. Cell 2 is a daughter cell that splits off of cell 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4ee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata for cell 0\n",
    "ex = tools.get_metadata(metadata_dict, structural_dict, 0)\n",
    "\n",
    "# Get metadata for cell 2\n",
    "ex2 = tools.get_metadata(metadata_dict, structural_dict, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f70212",
   "metadata": {},
   "source": [
    "We then need to create a series of offsets in order to properly align the ridgelines onto the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b08f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list of NA's is used to move the second cell into place so that its end aligns with the wave trough\n",
    "na_list = []\n",
    "for i in range(135):\n",
    "    na_list.append(np.nan)\n",
    "\n",
    "# These minimums are used to calculate offsets which align all of the first cells hieght profiles \n",
    "# Based on the location of te\n",
    "minimums = []\n",
    "for ridge in ex[\"Ridgeline\"]:\n",
    "    minimums.append(ridge.index(min(ridge)))\n",
    "offsets = []\n",
    "for mini in minimums:\n",
    "    offsets.append(minimums[-1]-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41118c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot by plotting each ridgeline on top of each other with the proper offsets. \n",
    "x = 0\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,20))\n",
    "for ridge1,ridge2, i in zip(ex[\"Ridgeline\"][14:27], ex2['Ridgeline'][14:27],range(len(ex[\"Ridgeline\"][14:27]))):\n",
    "    if np.isnan(ridge2).any() == True:\n",
    "        skip = []\n",
    "        for off in range(offsets[i]):\n",
    "            skip.append(np.nan)\n",
    "        ax.plot(np.array(skip+ridge1)+(x), color=\"k\")\n",
    "        x = x+500\n",
    "    else:\n",
    "        ax.plot(np.array(na_list+ridge2)+(x), color = \"r\")\n",
    "        x = x+500\n",
    "\n",
    "# Assign proper labels to all axes. \n",
    "ax.set_yticks([600])\n",
    "ax.set_ylabel(\"Height (nm)\", loc=\"bottom\", fontsize = 13)\n",
    "ax.set_xlabel(\"Length (pixels)\", fontsize = 13)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot()\n",
    "ax2.set_yticks([0], rotation=95)\n",
    "ax2.set_yticklabels([\"1\"], fontsize = 11)\n",
    "ax2.set_ylabel(\"Generation\", fontsize = 13)\n",
    "ax2.tick_params('y', colors='k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
